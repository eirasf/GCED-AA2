{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57knM8jrYZ2t"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eirasf/GCED-AA2/blob/main/lab1/p1-2.ipynb)\n",
    "\n",
    "# Práctica 1: Redes neuronales desde cero - Parte 2 - PyTorch\n",
    "\n",
    "En esta segunda parte de la práctica vamos a utilizar PyTorch para implementar y entrenar la misma red neuronal que desarrollamos con Numpy en la parte 1.\n",
    "\n",
    "Necesitaremos, por tanto, la librería `torch` además de las ya utilizadas `numpy`, `pandas` y `seaborn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LkaimNJfYZ2w"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Establecemos una semilla aleatoria para que los resultados sean reproducibles en distintas ejecuciones\n",
    "np.random.seed(1234567)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargaremos el conjunto de datos `titanic`, tal como hicimos en la parte 1 de la práctica pero transformando `X` e `y` en tensores de `torch`. Obtendremos dos tensores (`vectores_x` y `etiquetas`) que serán los que utilizaremos posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def load_titanic():\n",
    "    # Cargamos el dataset Titanic desde seaborn\n",
    "    df = sns.load_dataset('titanic')\n",
    "\n",
    "    # 1️⃣ Selección de variables relevantes y limpieza\n",
    "    # Columnas que vamos a usar\n",
    "    cols = ['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked', 'alone']\n",
    "    df = df[cols].copy()\n",
    "\n",
    "    # Eliminamos filas con valores faltantes\n",
    "    df = df.dropna(subset=['age', 'embarked', 'fare'])\n",
    "\n",
    "    # 2️⃣ Separar etiquetas y características\n",
    "    y = df['survived'].to_numpy().astype(np.float32)        # etiquetas como float\n",
    "    X = df.drop(columns=['survived'])\n",
    "\n",
    "    # 3️⃣ One-hot encoding para todas las variables categóricas\n",
    "    categorical_cols = ['pclass', 'sex', 'embarked', 'alone']\n",
    "    X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)  # drop_first=True evita multicolinealidad\n",
    "\n",
    "    # 4️⃣ Variables numéricas\n",
    "    numeric_cols = ['age', 'sibsp', 'parch', 'fare']\n",
    "    X_numeric = X_encoded[numeric_cols + [c for c in X_encoded.columns if c not in numeric_cols]]\n",
    "    scaler = StandardScaler()\n",
    "    X_numeric[numeric_cols] = scaler.fit_transform(X_numeric[numeric_cols])\n",
    "\n",
    "    # 5️⃣ Convertir a numpy arrays\n",
    "    X_np = X_numeric.to_numpy().astype(np.float32)\n",
    "    y_np = y.reshape(-1, 1).astype(np.float32)  # reshape para que sea (n_samples,1)\n",
    "\n",
    "    return X_np, y_np\n",
    "\n",
    "vectores_x, etiquetas = load_titanic()\n",
    "vectores_x = torch.from_numpy(vectores_x)\n",
    "etiquetas = torch.from_numpy(etiquetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaración del modelo\n",
    "\n",
    "En primer lugar, debemos crear en TensorFlow el grafo de operaciones que representa nuestro modelo. Para ello:\n",
    " 1. Creamos las variables que TF optimizará, es decir, los parámetros del modelo.\n",
    " 1. Creamos el grafo de operaciones que producen la predicción a partir de la entrada y las variables. En este caso utilizaremos funciones que relacionen variables de TF con tensores que contendrán datos utilizando operaciones de TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables auxiliares\n",
    "tamano_entrada = vectores_x.shape[1]\n",
    "h0_size = 5\n",
    "h1_size = 3\n",
    "\n",
    "# CREACIÓN DE LAS VARIABLES\n",
    "# TODO - Completa las dimensiones de las matrices\n",
    "W0 = torch.tensor(np.random.randn(h0_size, tamano_entrada), dtype=torch.float32, requires_grad=True)\n",
    "b0 = torch.tensor(np.random.randn(1, h0_size), dtype=torch.float32, requires_grad=True)\n",
    "W1 = torch.tensor(np.random.randn(h1_size, h0_size), dtype=torch.float32, requires_grad=True)\n",
    "b1 = torch.tensor(np.random.randn(1, h1_size), dtype=torch.float32, requires_grad=True)\n",
    "W2 = torch.tensor(np.random.randn(1, h1_size), dtype=torch.float32, requires_grad=True)\n",
    "b2 = torch.tensor(np.random.randn(1, 1), dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# Guardamos todas las variables en una lista para posteriormente acceder a ellas fácilmente\n",
    "VARIABLES = [W0, b0, W1, b1, W2, b2]\n",
    "\n",
    "\n",
    "# CREACIÓN DEL GRAFO DE OPERACIONES\n",
    "def capa_sigmoide(x, W, b):\n",
    "    # TODO - Completa con funciones de tensorflow el cálculo de la salida de una capa en la siguiente línea\n",
    "    return \n",
    "\n",
    "def predice(x):\n",
    "    # TODO - Completa las siguientes líneas\n",
    "    h0 = \n",
    "    h1 = \n",
    "    y = \n",
    "    return y\n",
    "\n",
    "# Verificación\n",
    "x_test = np.random.randn(1,tamano_entrada)\n",
    "y_pred = predice(x_test) \n",
    "print(y_pred)\n",
    "np.testing.assert_almost_equal(0.494716, y_pred.detach().numpy(), err_msg='Revisa tu implementación')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo\n",
    "El modelo declarado ya se puede utilizar para hacer predicciones pasándole a la función `predice` un tensor con datos (tal como se ha hecho en el apartado de verificación de la celda anterior). Sin embargo, como vimos en la parte 1, este modelo no está ajustado a los datos de entrada, por lo que producirá malas predicciones.\n",
    "\n",
    "Debemos encontrar un conjunto de valores para los parámetros ($\\mathbf{W}_2$, $b_2$, $\\mathbf{W}_1$, $\\mathbf{b}_1$, $\\mathbf{W}_0$ y $\\mathbf{b}_0$) que minimicen la función de coste. TensorFlow nos ayuda a optimizar este proceso.\n",
    "\n",
    "TensorFlow permite configurar el proceso de optimización, por lo que deberemos indicarle:\n",
    " 1. Qué función de pérdida queremos. En nuestro caso habíamos elegido la entropía cruzada binaria.\n",
    " 1. Qué método de optimización utilizar. Como en la parte 1, utilizaremos descenso de gradiente.\n",
    " \n",
    "Por el momento crearemos sendas variables para almacenar ambas configuraciones. Al estar organizado de esta manera, utilizar una función de pérdida distinta o un algoritmo de optimización diferente será tan sencillo como cambiar estas variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Función de pérdida (Binary Cross Entropy)\n",
    "fn_perdida = nn.BCELoss()  # espera que las salidas sean entre 0 y 1\n",
    "\n",
    "# Optimizador SGD con learning rate 0.1\n",
    "# VARIABLES es la lista de tensores con requires_grad=True\n",
    "optimizador = optim.SGD(VARIABLES, lr=0.1)\n",
    "# optimizador = optim.Adam(VARIABLES, lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El bucle de entrenamiento\n",
    "\n",
    "El bucle de entrenamiento será análogo al utilizado en la parte 1. Consistirá en ejecutar un número preestablecido (`NUM_EPOCHS`) de pasos de entrenamiento. En cada paso haremos lo siguiente:\n",
    " 1. Tomar los datos de entrada y calcular las predicciones que hace el modelo en su estado actual\n",
    " 1. Calcular el coste (la media de las pérdidas de cada predicción)\n",
    " 1. Utilizar el valor de coste para actualizar cada variable en dirección de su gradiente\n",
    "\n",
    "Crearemos una función `paso_entrenamiento` que realice este trabajo. PyTorch se ocupará de calcular los gradientes y realizar las actualizaciones de las variables. Para calcular los gradientes, debemos llamar a la función `backward` sobre el tensor que contiene el valor que queremos optimizar. Una vez la hayamos llamado, podemos pedir al optimizador que dé un `step` en la dirección de descenso del gradiente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_elems = vectores_x.shape[0]\n",
    "\n",
    "def paso_entrenamiento(x, y):\n",
    "    # Aseguramos que los gradientes se inicialicen a cero\n",
    "    optimizador.zero_grad()\n",
    "    \n",
    "    # TODO - Completa la siguiente línea para que calcule las predicciones\n",
    "    y_pred = \n",
    "    \n",
    "    # Cálculo de la pérdida utilizando la función que hemos escogido anteriormente\n",
    "    perdida = fn_perdida(y, y_pred)\n",
    "\n",
    "    # Esto computa el gradiente de la pérdida con respecto a todos los tensores que tienen requires_grad = True\n",
    "    perdida.backward()\n",
    "    \n",
    "    # Realizar la actualización de las variables solo requiere esta llamada\n",
    "    optimizador.step()\n",
    "    \n",
    "    # Tasa de acierto (accuracy)\n",
    "    fallos = torch.abs(y.reshape(-1,1) - y_pred)\n",
    "    tasa_acierto = torch.sum(1 - fallos)\n",
    "    \n",
    "    # Devolvemos estos dos valores para poder mostrarlos por pantalla cuando estimemos conveniente\n",
    "    return (perdida, tasa_acierto)\n",
    "\n",
    "# PROCESO DE ENTRENAMIENTO\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):    \n",
    "    perdida, tasa_error = paso_entrenamiento(vectores_x, etiquetas)\n",
    "    \n",
    "    if epoch % 100 == 99:\n",
    "        print(\"Epoch:\", epoch, 'Pérdida:', perdida.numpy(), 'Tasa de acierto:', tasa_error.numpy()/num_elems)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El uso de PyTorch nos ha permitido abstraernos de los detalles de implementación y del cálculo de derivadas para centrarnos en la arquitectura de nuestro modelo."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "WBk0ZDWY-ff8"
   ],
   "name": "AA2 - Lab2 - Parte 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
