{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GxLyX9vdCnf"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eirasf/GCED-AA2/blob/main/lab7/lab7-parte2.ipynb)\n",
    "# Práctica 7: Residual neural networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zjhho6Jb6-j"
   },
   "source": [
    "### Pre-requisitos. Instalar paquetes\n",
    "\n",
    "Para la segunda parte de este Laboratorio 7 necesitaremos TensorFlow y TensorFlow-Datasets. Además, como habitualmente, fijaremos la semilla aleatoria para asegurar la reproducibilidad de los experimentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjX2mh1-GSga"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "#Fijamos la semilla para poder reproducir los resultados\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "seed=1234\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyTV4jfNeiRk"
   },
   "source": [
    "Además, cargamos también APIs que vamos a emplear para que el código quede más legible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o3-nLMx-enlq"
   },
   "outputs": [],
   "source": [
    "#API de Keras, modelo Sequential y la capa Dense \n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense \n",
    "#Para mostrar gráficas\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xd9GpQD5WVY4"
   },
   "source": [
    "### Carga del conjunto de datos\n",
    "\n",
    "De nuevo, seguimos empleando el conjunto *german_credit_numeric* ya empleado en los laboratorios anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpsZlN_mEj7D"
   },
   "outputs": [],
   "source": [
    "# TODO: Carga el conjunto german_credit como ds_train\n",
    "# Indica además un tamaño de batch de 128 y que se repita indefinidamente\n",
    "ds_train = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización del desvanecimiento del gradiente\n",
    "En este apartado visualizaremos los tamaños de los gradientes, como hicimos en la primera parte. Para ello mantenemos la declaración de `GradientLoggingSequentialModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientLoggingSequentialModel(tf.keras.models.Sequential):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # En la inicialización instanciamos una nueva variable en la que\n",
    "        # registraremos el historial de tamaños de gradientes de cada capa\n",
    "        self.gradient_history = {}\n",
    "    \n",
    "    def compile(self, **kwargs):\n",
    "        result = super().compile(**kwargs)\n",
    "        # Una vez sabemos la arquitectura, podemos inicializar la historia\n",
    "        # de gradientes de cada capa a una lista vacía.\n",
    "        for l in self.layers:\n",
    "            self.gradient_history[l.name] = []\n",
    "        return result\n",
    "        \n",
    "    def _save_gradients(self, gradients):\n",
    "        # A cada paso de entrenamiento llamaremos a esta función para que\n",
    "        # registre los gradientes.\n",
    "        # En la lista gradients se encuentran los gradientes de las distintas\n",
    "        # capas por orden. Cada capa l tendrá un número de gradientes que\n",
    "        # concidirá con l.trainable_variables.\n",
    "        # Teniendo esto en cuenta, recorremos los gradientes, calculamos su\n",
    "        # tamaño y guardamos la media de tamaños de cada capa en el histórico\n",
    "        i = 0\n",
    "        for layer in self.layers:\n",
    "            gradient_sizes = []\n",
    "            for lw in layer.trainable_variables:\n",
    "                g_size = np.linalg.norm(gradients[i].numpy())\n",
    "                gradient_sizes.append(g_size)\n",
    "                i += 1\n",
    "            mean_gradient_size = np.mean(gradient_sizes)\n",
    "            self.gradient_history[layer.name].append(mean_gradient_size)\n",
    "        \n",
    "    def train_step(self, data):\n",
    "        # Haremos un paso de entrenamiento personalizado basado en \n",
    "        # https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit#a_first_simple_example\n",
    "        # Dejaremos el ejemplo tal cual, añadiendo tan solo la llamada a\n",
    "        # _save_gradients una vez que disponemos de los gradientes\n",
    "        \n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        x, y = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            # Compute the loss value\n",
    "            # (the loss function is configured in `compile()`)\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        \n",
    "        # Llamada añadida para grabar los gradientes.\n",
    "        self._save_gradients(gradients)\n",
    "        \n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del bloque con conexión residual\n",
    "\n",
    "En una red neuronal *feed-forward* convencional, la salida de cada capa se utiliza como entrada de las capa siguiente.\n",
    "<img src=\"./img/resnet.png\" alt=\"Dos capas en una red FF\" width=\"400\"/>\n",
    "\n",
    "En contraste, en una ResNet se introducen bloques que incluyen conexiones residuales con el objetivo de favorecer la propagación de los gradientes.\n",
    "<img src=\"./img/resnet-2.png\" alt=\"Dos capas en una red FF con conexión residual\" width=\"400\"/>\n",
    "\n",
    "A pesar de que las ResNet se suelen utilizar con redes convolucionales, en este Laboratorio utilizaremos una red *feed-forward*.\n",
    "\n",
    "Para poder utilizar este tipo de bloques en nuestra arquitectura definiremos un nuevo tipo de modelo denominado `DoubleDenseWithSkipModel` que consistirá en lo representado en la imagen:\n",
    " - Una primera capa Dense, cuya salida sirve de entrada a...\n",
    " - Una segunda capa Dense lineal (sin activación), cuya salida sirve de entrada a...\n",
    " - A una operación suma que la añade a la entrada original a la primera capa. La salida de esta suma servirá de entrada a...\n",
    " - Una función de activación, cuya salida será la salida del bloque.\n",
    "\n",
    "Utilizaremos la función sigmoide como función de activación en ambos casos. La entrada y la salida del bloque deberán tener la misma dimensión para que se pueda realizar la suma. Por simplicidad, mantendremos la salida de la primera capa con la misma dimensión.\n",
    "\n",
    "Nuestra nueva clase heredará de `Model`, por lo que deberá implementar los métodos `build` y `call`. En celdas posteriores añadiremos estos bloques a un modelo `Sequential` como si fuesen capas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDenseWithSkipModel(tf.keras.models.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DoubleDenseWithSkipModel, self).__init__(**kwargs)\n",
    "\n",
    "    # TODO: Completa el método build\n",
    "    def build(self, input_shape):\n",
    "        ...\n",
    "        \n",
    "    # TODO: Completa el método skip\n",
    "    def call(self, x):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENqWLaw7fuxl"
   },
   "source": [
    "## Creamos un modelo *GradientLoggingSequentialModel* utilizando las nuevas capas\n",
    "Creamos un modelo *GradientLoggingSequentialModel* para ajustar a los datos de entrada siguiendo las especificaciones dadas. Deberá incluir (además de las capas de entrada y salida) una capa de 10 unidades con activación sigmoide y 10 de los nuevos bloques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KrIOT-4DmRuu",
    "outputId": "79dba569-e5e5-426f-9a87-57757db7d363"
   },
   "outputs": [],
   "source": [
    "# TODO - Define en model una red GradientLoggingSequentialModel\n",
    "# Pon una capa densa con 10 unidades con activación sigmoide y 10 capas DoubleDenseWithSkipModel\n",
    "model = ...\n",
    "\n",
    "#Construimos el modelo y mostramos \n",
    "model.build()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo\n",
    "Como en la primera parte, vamos a establecer la función de pérdida (entropía cruzada binaria), el optimizador (SGD con LR $10^{-3}$) y la métrica que nos servirá para evaluar el rendimiento del modelo entrenado (área bajo la curva)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - Compila el modelo. Utiliza la opción run_eagerly=True para que se puedan registrar los gradientes a cada paso\n",
    "model.compile(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rx2nvo6G0Qvn"
   },
   "source": [
    "Entrenamos el modelo usando model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0EzQDyKfGfuu"
   },
   "outputs": [],
   "source": [
    "#TODO - entrenar el modelo utilizando 8 steps por epoch. Con 10 epochs nos valdrá para comprobar el desvanecimiento de gradientes.\n",
    "model.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pcjua_KlWymG"
   },
   "source": [
    "Una vez hayas encontrado un valor de learning rate que consiga una convergencia rápida, guarda el history de la pérdida en la variable history_sgd para poder hacer comparativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=(14, 6), dpi=80)\n",
    "pyplot.boxplot(model.gradient_history.values())\n",
    "pyplot.yscale('log')\n",
    "pyplot.xticks(ticks=range(1,len(model.gradient_history)+1), labels=model.gradient_history.keys())\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparativa\n",
    " - Contrasta los resultados con los obtenidos en la primera parte\n",
    " - Cambia las activaciones del bloque y de la primera capa oculta de la red a ReLU y observa la diferencia.\n",
    " - Alarga el entrenamiento y prueba distintos optimizadores para intentar que el modelo entrene correct"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab4_parte1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
