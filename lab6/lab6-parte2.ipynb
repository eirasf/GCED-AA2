{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GxLyX9vdCnf"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eirasf/GCED-AA2/blob/main/lab6/lab6-parte2.ipynb)\n",
    "# Práctica 6: Redes neuronales convolucionales - Complicando la CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zjhho6Jb6-j"
   },
   "source": [
    "### Pre-requisitos. Instalar paquetes\n",
    "\n",
    "Para la primera parte de este Laboratorio 6 necesitaremos TensorFlow y TensorFlow-Datasets. Además, como habitualmente, fijaremos la semilla aleatoria para asegurar la reproducibilidad de los experimentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjX2mh1-GSga"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "#Fijamos la semilla para poder reproducir los resultados\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "seed=1234567\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyTV4jfNeiRk"
   },
   "source": [
    "Además, cargamos también APIs que vamos a emplear para que el código quede más legible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o3-nLMx-enlq"
   },
   "outputs": [],
   "source": [
    "#API de Keras, modelo Sequential y la capa Dense \n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense \n",
    "#Para mostrar gráficas\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xd9GpQD5WVY4"
   },
   "source": [
    "### Carga del conjunto de datos\n",
    "\n",
    "En esta ocasión trabajaremos con el conjunto de imágenes *cifar10*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "VpIgeACxjG87",
    "outputId": "f71add90-62b7-4266-fc3f-26b369a115d7"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# El parámetro with_info=True nos permite acceder a información sobre el dataset\n",
    "# Carga el conjunto de datos mnist. Usaremos el primer 80% de la partición train para ds_train y el 20% restante para ds_val. ds_test tomará la partición test.\n",
    "(ds_train, ds_test, ds_val), ds_info = tfds.load(..., with_info=True, as_supervised=True)\n",
    "\n",
    "\n",
    "# En dicha información se encuentran los nombres de las clases y las dimensiones de las imágenes\n",
    "NUM_CLASSES = ds_info.features['label'].num_classes\n",
    "nombres_clases = ds_info.features['label'].names\n",
    "dimensiones = ds_info.features['image'].shape\n",
    "\n",
    "# Para comprobar que se ha cargado tomamos un elemento y lo mostramos\n",
    "ej_imagen, ej_etiqueta = next(iter(ds_train.take(1)))\n",
    "pyplot.imshow(ej_imagen)\n",
    "pyplot.xlabel(nombres_clases[ej_etiqueta.numpy()])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENqWLaw7fuxl"
   },
   "source": [
    "\n",
    "## Preprocesado de los datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bi3yj3OCjG89"
   },
   "source": [
    "Nuevamente deberíamos convertir las etiquetas suministradas a codificación one_hot, pero vamos a tomar otra alternativa. Seguiremos prediciendo un vector con una componente por clase, pero mantendremos la etiqueta numérica y luego utilizaremos funciones de pérdida y métricas adaptadas a esta circunstancia. No obstante, seguimos teniendo que escalar los píxeles de la imagen al rango \\[0,1\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ...\n",
    "ds_val = ...\n",
    "ds_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8oR1FffxjG8_"
   },
   "source": [
    "## Creando el modelo\n",
    "\n",
    "Declara ahora un modelo convolucional con la siguiente arquitectura:\n",
    " 1. [Convolución 2D](https://keras.io/api/layers/convolution_layers/convolution2d/) de 32 filtros y stride 3, con activación ReLU\n",
    " 1. [Pooling 2D](https://keras.io/api/layers/pooling_layers/max_pooling2d/) tomando el máximo de cada grupo de 2x2\n",
    "  1. [Convolución 2D](https://keras.io/api/layers/convolution_layers/convolution2d/) de 64 filtros y stride 3, con activación ReLU\n",
    " 1. [Pooling 2D](https://keras.io/api/layers/pooling_layers/max_pooling2d/) tomando el máximo de cada grupo de 2x2\n",
    " 1. [Convolución 2D](https://keras.io/api/layers/convolution_layers/convolution2d/) de 64 filtros y stride 3, con activación ReLU\n",
    " 1. Capa Densa (requiere aplanado previo) de 64 unidades y activación ReLU\n",
    " 1. Capa de salida\n",
    " \n",
    "Ejecuta la siguiente celda y repite el compilado, entrenamiento y verificaciones posteriores para observar la diferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KrIOT-4DmRuu",
    "outputId": "cd790a45-4a08-467c-c64e-d65dd5a09eca"
   },
   "outputs": [],
   "source": [
    "# TODO - Crea el modelo descrito\n",
    "model = ...\n",
    "\n",
    "#Construimos el modelo y mostramos \n",
    "model.build()\n",
    "print(model.summary())\n",
    "\n",
    "# VERIFICACIÓN\n",
    "assert model.count_params()==122570, 'Revisa la arquitectura de tu modelo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNSojR94dP3y"
   },
   "source": [
    "### Entrenamiento del modelo\n",
    "Vamos a establecer la función de pérdida, el optimizador (Adam con el LR por defecto).\n",
    "\n",
    "Usaremos la entropía cruzada categórica (de nuevo con logits y utilizando la versión Sparse ya que las etiquetas son enteros).\n",
    "\n",
    "Como métrica repetiremos con la precisión categórica, pero también en su versión *sparse*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvgmlS2_jG9A"
   },
   "outputs": [],
   "source": [
    "#TODO - Compila el modelo con los parámetros indicados\n",
    "model.compile(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VdJzSONWgUU9"
   },
   "source": [
    "Como siempre, entrenaremos el modelo usando `model.fit`. Para ello, previamente debemos indicar a nuestro dataset que haga lotes de 128 elementos. Le indicaremos también que baraje los datos utilizando un buffer de 5 veces el tamaño de lote. La aleatorización debe hacerse antes de la partición en lotes, para que se aleatoricen los elementos y no los lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46ERVCeOrifg",
    "outputId": "a2c0d22f-fe47-4bf6-8625-0f419ea12415"
   },
   "outputs": [],
   "source": [
    "# TODO - Baraja y trocea los datasets en lotes.\n",
    "ds_train_batch = ...\n",
    "ds_val_batch = ...\n",
    "\n",
    "# TODO - Entrena el modelo. Con 20 epochs será suficiente.\n",
    "# Haz que nos ofrezca también las mediciones de pérdida y precisión sobre el conjunto de validación,\n",
    "# para saber si el modelo está sobreajustando.\n",
    "history = model.fit(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "id": "DhMlM_4-jG9B",
    "outputId": "641be78a-00ac-45e0-d8ec-a7c60aa12136"
   },
   "outputs": [],
   "source": [
    "# TODO - Muestra las gráficas de pérdida y precisión para entrenamiento y validación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rx2nvo6G0Qvn"
   },
   "source": [
    "### Verificación del rendimiento\n",
    "Aprovecharemos el conjunto de test para comprobar la capacidad de generalización de nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MufohUpFlb-n",
    "outputId": "27b7efd7-343c-4d42-dbbd-2f61c52dd8ba"
   },
   "outputs": [],
   "source": [
    "# TODO - Evalúa el modelo sobre el conjunto de test. Previamente deberás hacer lotes con el conjunto de test.\n",
    "print(\"Evaluación sobre el conjunto TEST:\")\n",
    "ds_test_batch = ...\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ExBje4wgvDE"
   },
   "source": [
    "Si todo ha ido bien, deberías haber obtenido una preción en test del 60%, lo cual no es desdeñable para una red sencilla.\n",
    "\n",
    "## Mejora del rendimiento\n",
    "Vuelve sobre la arquitectura del modelo e incluye capas de [BatchNormalization](https://keras.io/api/layers/normalization_layers/batch_normalization/) después de cada capa convolucional. Esto hará que las salidas de esa capa se estandaricen para que sigan una distribución N(0,1) (la operación de estandarización se incluye en el grafo y, por tanto, el cómputo de los gradientes). El efecto de esta capa es que se evitará que el gradiente del lote tenga una gran componente solo dedicada a acercar las salidas a la media/desviaición de las muestras en cada capa. En consecuencia, el aprendizaje se acelera.\n",
    "\n",
    "### Ejercicios\n",
    " - Repite el entrenamiento con la nueva arquitectura. ¿Qué efecto has notado? ¿Puedes mejorar el rendimiento en test utilizando conceptos vistos en Laboratorios anteriores?\n",
    " - Prueba distintas arquitecturas e intenta mejorar el rendimiento en test."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lab6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
