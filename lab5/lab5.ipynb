{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GxLyX9vdCnf"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eirasf/GCED-AA2/blob/main/lab5/lab5.ipynb)\n",
    "# Práctica 5: Optimización de redes neuronales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zjhho6Jb6-j"
   },
   "source": [
    "En este laboratorio trabajaremos con el mismo conjunto de datos que hemos utilizado hasta ahora. Carga todo el conjunto en un solo dataloader (en este laboratorio solo nos ocuparemos de estudiar el ajuste en entrenamiento, no la generalización)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjX2mh1-GSga"
   },
   "outputs": [],
   "source": [
    "# TODO - Carga los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNSojR94dP3y"
   },
   "source": [
    "Usaremos la misma arquitectura, función para el aprendizaje, función para evaluación y función de coste que en los laboratorios anteriores. Haz las declaraciones necesarias en la siguiente celda.\n",
    "\n",
    "Adapta la función `train_model` para que el parámetro `val_loader` pueda ser `None`, en cuyo caso no se calcula la pérdida en validación y se devuelve una lista vacía en la segunda componente de la tupla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oXz3n9daGXjG"
   },
   "outputs": [],
   "source": [
    "# TODO - Declara las funciones, la arquitectura y la función de pérdida a utilizar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con todo esto listo, haremos una comparativa de los distintos optimizadores que nos ofrece `torch`.\n",
    "\n",
    "En primer lugar haremos un entrenamiento con un optimizador SGD sin momentum. Instancia el modelo y un optimizador `optim.SGD`. Haz el entrenamiento y muestra la curva de aprendizaje. Prueba distintos valores de *learning rate*. Guarda las curvas de aprendizaje en un diccionario para mostrarlas todas en un mismo gráfico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs_a_explorar = [0.1, 0.01, 0.001] # TODO - Cambia esta lista para incluir los valores que consideres\n",
    "histories = {}\n",
    "\n",
    "for lr in lrs_a_explorar:\n",
    "    model = ...\n",
    "    optimizer = ...\n",
    "    train_losses, _ = ...\n",
    "    train_accuracy = ...\n",
    "\n",
    "    # TODO: Guarda el history del entrenamiento\n",
    "    histories[f'SGD_{lr}'] = {'loss_history': train_losses, 'accuracy_history': train_accuracy}\n",
    "\n",
    "# Mostramos en gráficas las curvas de aprendizaje que tengamos acumuladas en histories\n",
    "for k in histories:\n",
    "    pyplot.plot(histories[k]['loss_history'], label=k)\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparativas\n",
    "\n",
    "Repite todo el proceso de entrenamiento realizando cada uno de estos cambios. Haz varias pruebas hasta encontrar valores adecuados. No resetees el kernel ni vuelvas a ejecutar las dos primeras celdas entre las distintas pruebas, o perderás la posibilidad de comparar las curvas de aprendizaje. Recuerda modificar la etiqueta de la curva de cada prueba en la gráfica cuando añadas registros a `histories`.\n",
    "\n",
    "1. **Tamaño del lote**: Modifica el tamaño de lote al crear el dataloader y compara los resultados\n",
    "1. **Learning rate variable**: Vuelve sobre la celda en la que se declara el optimizador y utiliza un [scheduler](https://docs.pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) para que el *learning rate* vaya decayendo linealmente con el tiempo. Utiliza los consejos vistos en teoría para elegir los parámetros. Tendrás que modificar tu función de aprendizaje para que acepte el `scheduler` como parámetro y que llame a su método `step` cuando sea necesario. También es útil mostrar por pantalla el *learning rate* en cada epoch.\n",
    "1. **Uso de momentum**: Vuelve sobre la celda en la que se declara el optimizador y configúralo para que utilice momentum. Consulta los apuntes y la [documentación](https://docs.pytorch.org/docs/stable/generated/torch.optim.SGD.html#sgd) para ayudarte a elegir los hiperparámetros\n",
    "1. **Otros optimizadores**: Ayúdate de la [documentación](https://docs.pytorch.org/docs/stable/optim.html#algorithms) para probar RMSProp, Adagrad y Adam y compara con los resultados obtenidos hasta ahora.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab4_parte1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "aa2-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
