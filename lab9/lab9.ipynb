{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "9_rnns.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57knM8jrYZ2t"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/15w47aHUM52MCJViF_LXTuAGFczg29J0g?usp=sharing)\n",
        "\n",
        "# Práctica 9: RNNs\n",
        "\n",
        "## Pre-requisitos\n",
        "\n",
        "### Instalar paquetes\n",
        "\n",
        "Si la práctica requiere algún paquete de Python, habrá que incluir una celda en la que se instalen. Si usamos un paquete que se ha utilizado en prácticas anteriores, podríamos dar por supuesto que está instalado pero no cuesta nada satisfacer todas las dependencias en la propia práctica para reducir las dependencias entre ellas.\n",
        "\n",
        "### NOTA: En <font color='red'>Google Colab</font> hay que instalar los paquetes EN CADA EJECUCIÓN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkaimNJfYZ2w"
      },
      "source": [
        "# Ejemplo de instalación de tensorflow 2.0\n",
        "#%tensorflow_version 2.x\n",
        "# !pip3 install tensorflow  # NECESARIO SOLO SI SE EJECUTA EN LOCAL\n",
        "import tensorflow as tf\n",
        "\n",
        "# Hacemos los imports que sean necesarios\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import os\n",
        "import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOch-CnwQttl"
      },
      "source": [
        "# Pronóstico de series de tiempo\n",
        "\n",
        "Vamos a utilizar un [conjunto de datos de series de tiempo meteorológicas](https://www.bgc-jena.mpg.de/wetter/) registradas por el Instituto Max Planck de Biogeoquímica. \n",
        "\n",
        "Este conjunto de datos contiene 14 características diferentes, como la temperatura del aire, la presión atmosférica y la humedad. Estos se recopilaron cada 10 minutos, de 2009 a 2016.\n",
        "\n",
        "Lo primero que vamos a hacer es cargar los datos en un DataFrame de la librería [Pandas](https://pandas.pydata.org/).\n",
        "\n",
        "En Keras tenemos implementado tanto las capas recurrentes [GRU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU) y [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM). Sin embargo, es posible crear cualquier capa recurrente que queramos de una manera sencilla. Basta con crear una capa (llamada celda) que indique las operaciones a realizar, y luego encapsularla en la capa [RNN](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN). \n",
        "\n",
        "Por tanto, en esta clase vamos a tratar de replicar la celda de la capa GRU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gXdWDBIEKel"
      },
      "source": [
        "zip_path = tf.keras.utils.get_file(\n",
        "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
        "    fname='jena_climate_2009_2016.csv.zip',\n",
        "    extract=True)\n",
        "csv_path, _ = os.path.splitext(zip_path)\n",
        "df = pd.read_csv(csv_path)[::6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkaCDOGapMyl"
      },
      "source": [
        "## Visualización de los datos\n",
        "\n",
        "Antes de nada, vamos a echar un vistazo a los datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xjcLa21EKen"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHwD7iLeWTm1"
      },
      "source": [
        "También podemos ver como cambian los datos de las variables a lo largo del tiempo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aoHeJUGEKeo"
      },
      "source": [
        "plot_cols = ['T (degC)', 'p (mbar)', 'rho (g/m**3)']\n",
        "plot_features = df[plot_cols]\n",
        "plot_features.index = df['Date Time']\n",
        "_ = plot_features.plot(subplots=True)\n",
        "\n",
        "plot_features = df[plot_cols][:480]\n",
        "plot_features.index = df['Date Time'][:480]\n",
        "_ = plot_features.plot(subplots=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBWw_ncKh1EA"
      },
      "source": [
        "## Preprocesado de los datos\n",
        "\n",
        "Primero de nada, vamos a observar las estadísticas de cada dato. Para ello, la librería Pandas nos permite obtener información de una manera muy sencilla."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFyHhz4lhvme"
      },
      "source": [
        "df.describe().transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cpyBP_MZdj-"
      },
      "source": [
        "### Limpiar el dataset\n",
        "En los datos podemos ver cosas extrañas. Por ejemplo, en la velocidad **wv** tenemos, como dato mínimo, valores de -9999m/s, que es una velocidad absurda. En este caso tenemos dos opciones:\n",
        "\n",
        "1. Eliminar las filas con el error.\n",
        "1. Poner un dato más fiable.\n",
        "\n",
        "En nuestro caso, vamos a decidirnos por la última, sustituyendo cualquier valor menor que 0 por un 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCuqvBRlXcSa"
      },
      "source": [
        "## TODO: sustituye los datos negativos de las columnas wv (m/s) y max. wv (m/s) por 0's\n",
        "wv = df['wv (m/s)']\n",
        "wv_negativos = None\n",
        "wv[wv_negativos] = None\n",
        "\n",
        "wv_max = None\n",
        "None\n",
        "None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77mBj__rcrui"
      },
      "source": [
        "Si la operación se ha completado satisfactoriamente, puedes volver a evaluar las estadísticas del dataset para comprobar que se han modificado los valores. \n",
        "\n",
        "### Transformación de datos\n",
        "\n",
        "So observamos los datos vemos que tenemos ciertos componentes que necesitan ser preprocesados para evitar errores innecesarios.\n",
        "\n",
        "#### Viento\n",
        "\n",
        "El viento se compone de las 3 últimas variables. En especial la última, **wd (deg)**, que marca la dirección del viento. ¿Cuál es el problema que contiene esta variable? Tómate un tiempo para evaluar la problemática."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC1htvSroc7s"
      },
      "source": [
        "<font color='red'>TODO:</font> Escribe aquí el motivo por el que crees que la variable **wd (deg)** puede ser problemática en su forma actual.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hSCpabQMlnR"
      },
      "source": [
        "¿Ya te has dado cuenta del problema? ¡Es hora de arreglarlo! Sustituye el dato por una representación más útil. \n",
        "\n",
        "<font color='red'>PISTA:</font> la solución implica dividir la variable en dos nuevos campos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXNa7oG8M9o1"
      },
      "source": [
        "# Convertimos los grados a radianes\n",
        "wd_rad = df.pop('wd (deg)')*np.pi / 180\n",
        "\n",
        "# TODO: modifica wd (deg) para obtener una mejor representación\n",
        "# Crea los nuevos componentes e insértalos en el dataframe.\n",
        "None\n",
        "None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMYrV16rFHEZ"
      },
      "source": [
        "#### Fecha\n",
        "\n",
        "La fecha es también algo muy importante en los datos, pero hay que tener en cuenta su periodicidad. En concreto, dos casos:\n",
        "\n",
        "- Periodicidad en la hora del día. \n",
        "- Periodicidad en la época del año.\n",
        "\n",
        "Un método simple para convertirlo en una señal utilizable es usar el [seno](https://numpy.org/doc/stable/reference/generated/numpy.sin.html) y el [coseno](https://numpy.org/doc/stable/reference/generated/numpy.cos.html) para convertir la hora en señales claras de \"Hora del día\" y \"Hora del año\". Hay que operar de tal manera que:\n",
        "\n",
        "1. $\\cos(hora~0) = \\cos(hora~24)$ y $\\sin(hora~0) = \\sin(hora~24)$ en la hora del día.\n",
        "1. $\\cos(1~enero) \\approx \\cos(31~diciembre)$ y $\\sin(1~enero) \\approx \\sin(31~diciembre)$ en la hora anual.\n",
        "\n",
        "<font color='red'>PISTA:</font> recuerda que:\n",
        "- $\\cos(0) = \\cos(2\\pi)$.\n",
        "- $\\sin(0) = \\sin(2\\pi)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42A3_NTSFiUb"
      },
      "source": [
        "# Vamos a quitar la columna de la fecha\n",
        "date_time = pd.to_datetime(df.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')\n",
        "\n",
        "# Lo convertimos en segundos\n",
        "timestamp_s = date_time.map(pd.Timestamp.timestamp)\n",
        "\n",
        "# TODO: calcula el número de segundos de un día y de un año\n",
        "day = None\n",
        "year = None\n",
        "\n",
        "# TODO: Extrae, para cada fecha, el seno y el coseno, y añádelos al dataframe\n",
        "df['Dia sin'] = None\n",
        "df['Dia cos'] = None\n",
        "df['Anho sin'] = None\n",
        "df['Anho cos'] = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PD6nr0jR4wX"
      },
      "source": [
        "## Partición de datos\n",
        "\n",
        "Usaremos una división **(70%, 20%, 10%)** para los conjuntos de entrenamiento, validación y prueba. Tenga en cuenta que los datos **no** se mezclan aleatoriamente antes de dividirlos. Esto es por dos razones:\n",
        "\n",
        "1. Garantiza que aún sea posible dividir los datos en ventanas de muestras consecutivas.\n",
        "1. Garantiza que los resultados de la validación / prueba sean más realistas y se evalúen en función de los datos recopilados después de que el modelo haya sido entrenado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx1-NUN5SOZZ"
      },
      "source": [
        "# TODO: parte los datos según el porcentaje estipulado\n",
        "n_datos = len(df)\n",
        "\n",
        "train_df = None\n",
        "val_df = None\n",
        "test_df = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xTEC4-YajTQ"
      },
      "source": [
        "# Normalización de los datos\n",
        "\n",
        "Para que una red neuronal entrene correctamente suele ser necesario que todos los datos tengan una misma escala, o al menos una parecida. Sin embargo, anteriormente hemos visto como algunas variables tienen medias cercanas a 1000, mientras que otras rondan el 0. Por tanto, necesitamos igualarlas.\n",
        "\n",
        "Existen múltiples maneras de escalar los datos, pero una de las más comunes es la normalización:\n",
        "\n",
        "$$ \\tilde{x} = \\dfrac{x - mean(x)}{std(x)} $$\n",
        "\n",
        "Consiste en restar, **a cada columna**, su media, dividiendo el resultado por su desviación típica. Para realizarlo, podemos hacer uso de las funciones de Pandas [mean](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mean.html) y [std](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.std.html).\n",
        "\n",
        "Otra cosa a tener en cuenta es que **se usa la media y desviación del conjunto de entrenamiento** para normalizar todos los datos. Esto es así porque es el único conjunto de datos que conoceríamos a priori en un problema real."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxm2jjJ6b7Fm"
      },
      "source": [
        "# TODO = normaliza los dataframes\n",
        "train_mean = None\n",
        "train_std = None\n",
        "\n",
        "train_df = None\n",
        "val_df = None\n",
        "test_df = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7kRHezEdR3-"
      },
      "source": [
        "Ahora podemos observar como los datos están más o menos balanceados, y todos en la misma escala."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Za-P-Vh5ehE-"
      },
      "source": [
        "df_std = train_df.melt(var_name='Column', value_name='Normalized')\n",
        "plt.figure(figsize=(12, 6))\n",
        "ax = sns.violinplot(x='Column', y='Normalized', data=df_std)\n",
        "_ = ax.set_xticklabels(df.keys(), rotation=90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9yQSlWTyaCp"
      },
      "source": [
        "## Ventana de datos\n",
        "\n",
        "Los modelos de esta práctica harán un conjunto de predicciones basadas en una ventana de muestras consecutivas de los datos. La agrupación de los datos se determinará mediante 3 parámetros:\n",
        "\n",
        "1. **Anchura de la entrada (input_width):** Es el número de datos de diferentes tiempos que vamos a meter en el modelo. \n",
        "1. **Anchura de la salida (label_width):** Es el número de datos de diferentes tiempos que vamos a tratar de predecir. \n",
        "1. **Retardo de la salida (offset):** Es el tiempo a pasar entre el último dato de la entrada y el primero de la salida.\n",
        "\n",
        "Aquí podemos ver una serie de distintos ejemplos:\n",
        "\n",
        "***\n",
        "`input_width = 6, label_width = 1, offset = 1`\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1FJ4zNQO1k9mZoZUWgYEZV5QpYx78CuWP)\n",
        "***\n",
        "\n",
        "***\n",
        "`input_width = 24, label_width = 1, offset = 24`\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1aIRzPiPVVl_OLxYn-6FK0XRi3fkyi6BU)\n",
        "***\n",
        "\n",
        "El objetivo de esta parte es **crear una función que divida el dataset de entrada en todos los segmentos posibles de entrada/salida**, teniendo en cuenta los 3 parámetros mencionados anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAkmdOj9FdF4"
      },
      "source": [
        "# TODO: completa la función sliding_window, sustituyendo las partes con None\n",
        "\n",
        "def sliding_window(data, labels, input_width, label_width=1, offset=1):\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for i in range(None):\n",
        "        _x = data[i:None]\n",
        "        _y = labels[None]\n",
        "        x.append(_x)\n",
        "        y.append(_y)\n",
        "\n",
        "    x, y = np.array(x),np.array(y)\n",
        "\n",
        "    if len(x.shape) == 2:\n",
        "        x = x[:,:,np.newaxis]\n",
        "\n",
        "    if len(y.shape) == 2:\n",
        "        y = y[:,:,np.newaxis]\n",
        "    \n",
        "    return x, y\n",
        "\n",
        "\n",
        "# Prueba el código\n",
        "prueba_x, prueba_y = sliding_window(np.arange(5), np.arange(5), input_width=3, label_width=2, offset=2)\n",
        "print((prueba_x, prueba_y)) # La salida esperada es (array([[[0], [1], [2]], [[1], [2], [3]]]), array([[[2], [3]], [[3], [4]]]))\n",
        "assert(prueba_x.shape == (2, 3, 1) and prueba_y.shape == (2, 2, 1)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fXUE0t9H7fq"
      },
      "source": [
        "Una vez creada la función, se la pasamos a nuestros datos. Primero, definimos los parámetros de la función."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9AKEH2lIB_U"
      },
      "source": [
        "input_width = 24\n",
        "label_width = 24\n",
        "offset = -input_width + 1\n",
        "target_labels = 'T (degC)' # Vamos a tratar de predecir la temperatura\n",
        "\n",
        "\n",
        "# TODO: Llama a la función para dividir el dataset\n",
        "x_train, y_train = None\n",
        "x_val, y_val = None\n",
        "x_test, y_test = None\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHgGO3NCSKOo"
      },
      "source": [
        "Vamos a comprobar el tamaño de los datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcgmDl-QSNoE"
      },
      "source": [
        "print('x_train shape :', x_train.shape)\n",
        "print('y_train shape :', y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ttiMl9JlzBR"
      },
      "source": [
        "## Creando el modelo\n",
        "\n",
        "Una vez obtenido un correcto preprocesado de los datos, vamos a crear el modelo. Nuestro primer modelo constará de dos partes:\n",
        "\n",
        "1. Capa recurrente ([LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM))\n",
        "1. Capa densa, para adecuar la salida al tamaño esperado.\n",
        "\n",
        "Un argumento de constructor importante para todas las capas de keras RNN (como LSTM) es el argumento `return_sequences`. Este parámetro puede configurar la capa de dos formas.\n",
        "\n",
        "1. Si es `False` (el valor por defecto) la capa solo devuelve el resultado del paso de tiempo final, lo que le da tiempo al modelo para calentar su estado interno antes de hacer una sola predicción:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1-pzuZXOF_pratYFLyJRCg_oKZRsKdzbA)\n",
        "\n",
        "2. Si es `True` la capa devuelve una salida para cada entrada:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=17SYlF1UqHdCTTxZangKABGEwd_Mse44p)\n",
        "\n",
        "Esto es útil para:\n",
        "  * Apilamiento de capas RNN.\n",
        "  * Entrenamiento de un modelo en múltiples pasos de tiempo simultáneamente.\n",
        "\n",
        "En nuestro caso, hemos definido nuestros datos con solapamiento, con el objetivo de entrenar nuestro modelo en múltiples pasos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhM3Sup5dcb_"
      },
      "source": [
        "# TODO: Completa el modelo con las partes necesarias\n",
        "\n",
        "lstm_model = tf.keras.models.Sequential([\n",
        "    None, # Capa recurrente (usa un múmero de unidades de la capa oculta de 32, y establece correctamente el valor de return_sequences).\n",
        "    None # Capa densa (escoge correctamente su número de unidades)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qwFar8Eg3bQ"
      },
      "source": [
        "## Entrenando el modelo\n",
        "\n",
        "Por último, sólo nos queda entrenar el modelo durante 20 iteraciones. Vamos a utilizar un `callback` llamado [`EarlyStopping`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) para que pare el entrenamiento si el resultado en validación no mejora pasadas una serie de iteraciones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuFMXGGwfFRT"
      },
      "source": [
        "MAX_EPOCHS = 20\n",
        "batch_size = 32\n",
        "\n",
        "def entrenar_modelo(model, train_data, train_label, val_data, val_label, epochs, batch_size, patience=5):\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                      patience=patience,\n",
        "                                                      mode='min')\n",
        "\n",
        "    model.compile(loss=tf.losses.MeanSquaredError(),\n",
        "                  optimizer=tf.optimizers.Adam(),\n",
        "                  metrics=[tf.metrics.MeanAbsoluteError()])\n",
        "\n",
        "    history = model.fit(train_data, train_label, epochs=epochs,\n",
        "                        batch_size=batch_size,\n",
        "                        validation_data=(val_data, val_label),\n",
        "                        callbacks=[early_stopping])\n",
        "    return history\n",
        "\n",
        "history = entrenar_modelo(lstm_model, x_train, y_train, x_val, y_val, MAX_EPOCHS, batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBZWT9Aa23su"
      },
      "source": [
        "### Visualizando el resultado\n",
        "\n",
        "Una vez entrenado el modelo, vamos a ver cómo hace la predicción en test, con respecto al resultado esperado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cnyk2jr3D8u"
      },
      "source": [
        "def plot_prediction(feed_data, expected_result, model_result, target_label, offset=1, index=0):\n",
        "\n",
        "    f_data_x = np.arange(feed_data.shape[1])\n",
        "    e_data_x = np.arange(expected_result.shape[1]) + offset\n",
        "    plt.plot(f_data_x, feed_data[index])\n",
        "    plt.plot(e_data_x, expected_result[index], '*r')\n",
        "    plt.plot(e_data_x, model_result[index], '^g')\n",
        "    plt.legend(['inputs', 'labels', 'predictions'])\n",
        "    plt.title(target_label)\n",
        "\n",
        "# TODO: predecimos el resultado del test con el modelo\n",
        "model_result = None\n",
        "\n",
        "target_index = test_df.columns.get_loc(target_labels)\n",
        "\n",
        "# TODO: llama a la función plot_prediction con los parámetros correctos\n",
        "plot_prediction(x_test[:, :, target_index], None, None, target_labels, offset, index=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5doswPU8oIj"
      },
      "source": [
        "La predicción es bastante cercana...pero algo falla. **¡Las temperaturas están mostrando el resultado normalizado!** Revierte la normalización para un correcto visualizado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hKwoekP9OTe"
      },
      "source": [
        "# TODO: llama a la función plot_prediction con los valores sin normalizar\n",
        "# None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "souHxxK3_8Np"
      },
      "source": [
        "¡Enhorabuena! Has creado tu primera red recursiva. Ahora vamos a hacerlo un poco más complejo.\n",
        "\n",
        "# Predicción de múltiples pasos a futuro\n",
        "\n",
        "El modelo que vimos anteriormente utiliza la salida de cada entrada del LSTM para predecir el tiempo siguiente. \n",
        "\n",
        "**¿Qué pasaría entonces si uso el mismo modelo para predecir la temperatura en las 24h siguientes?** Si usásemos el mismo modelo, la predicción de la temperatura en el la hora 25 sólo tomaría en cuenta los datos de la temperatura de la primera hora, en lugar de las 24h que tenemos registradas. Por tanto, tendremos que hacer unos cambios en nuestro modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrfJjbD1A8Cf"
      },
      "source": [
        "input_width = 24\n",
        "label_width = 24\n",
        "offset = 1\n",
        "target_labels = 'T (degC)' # Vamos a tratar de predecir la temperatura\n",
        "\n",
        "\n",
        "# TODO: Llama a la función para dividir el dataset\n",
        "x_train, y_train = None\n",
        "x_val, y_val = None\n",
        "x_test, y_test = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu5xtqQ5BSWA"
      },
      "source": [
        "## Creando el modelo\n",
        "\n",
        "En este modelo vamos a necesitar acumular la información de las 24h de entrada para empezar a predecir los resultados a la salida. El esquema a seguir es el siguiente:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1R1tGHuXo2yueWlNyP1tWfgQLM6YP1xPw)\n",
        "\n",
        "Por tanto, debemos hacer un cambio en el argumento `return_sequences` que explicamos anteriormente.\n",
        "\n",
        "A mayores, lo que vamos a hacer es concatenar dos capas LSTM. Para concatenar capas recurrentes, lo que hay que tener en cuenta es que el parámetro `return_sequences=True` tiene que estar activado en todas las capas menos en la última. En dicha capa, el valor de `return_sequences` dependerá de lo que queramos obtener."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrSbFYGzCcR8"
      },
      "source": [
        "# TODO: Completa el modelo con las partes necesarias\n",
        "\n",
        "lstm_model = tf.keras.models.Sequential([\n",
        "    None, # Capa recurrente (usa un múmero de unidades de la capa oculta de 32, y establece correctamente el valor de return_sequences).\n",
        "    None, # Capa recurrente (usa un múmero de unidades de la capa oculta de 32, y establece correctamente el valor de return_sequences).\n",
        "    None # Capa densa (escoge correctamente su número de unidades, \n",
        "#                      ya que se necesita una unidad por cada característica de salida que queramos predecir, \n",
        "#                      y por el número de veces que necesitamos predecir esa característica),\n",
        "    tf.keras.layers.Reshape((24, 1)) # reorganizamos la salida para que sea del tipo (label_width, n_output_features)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beQs706hD7O2"
      },
      "source": [
        "## Entrenando el modelo\n",
        "\n",
        "Por último, sólo nos queda entrenar el modelo durante 20 iteraciones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgdHO_c6EBAA"
      },
      "source": [
        "history = entrenar_modelo(lstm_model, x_train, y_train, x_val, y_val, MAX_EPOCHS, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPAdleFHEYPR"
      },
      "source": [
        "### Visualizando el resultado\n",
        "\n",
        "Una vez entrenado el modelo, vamos a ver cómo hace la predicción en test, con respecto al resultado esperado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvJDEPbAEnPl"
      },
      "source": [
        "# TODO: predecimos el resultado del test con el modelo\n",
        "model_result = None\n",
        "\n",
        "target_index = test_df.columns.get_loc(target_labels)\n",
        "\n",
        "# TODO: llama a la función plot_prediction con los parámetros correctos\n",
        "plot_prediction(None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIEsuejQv4mP"
      },
      "source": [
        "Como puedes ver, es muchísimo más difícil predecir el tiempo con mucha antelación.\n",
        "\n",
        "# ¡ENHORABUENA! Has completado la práctica de Redes recurrentes.\n",
        "\n",
        "# ¿Deseas saber más?\n",
        "\n",
        "La redes recursivas también permiten la predicción a posteriori, utilizando como datos de entradas las mismas predicciones de la red en tiempos anteriores. Son las llamadas [redes auto-regresivas](https://eigenfoo.xyz/deep-autoregressive-models/). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QIu2fE3tIHA"
      },
      "source": [
        "# Trabajo extra\n",
        "\n",
        "Como trabajo extra, se propone realizar una idea parecida, pero con dificultad añadida.\n",
        "\n",
        "- Se utilizará una capa [GRU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU) en lugar de la LSTM.\n",
        "- Se propone predecir todas las variables del dataset al mismo tiempo, en lugar de sólo la temperatura."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMavrtMX_7V7"
      },
      "source": [
        "# TODO: escribe el código para el trabajo extra sin ayuda. Usa todos los bloques de código que quieras."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}