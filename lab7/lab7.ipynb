{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GxLyX9vdCnf"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eirasf/GCED-AA2/blob/main/lab7/lab7.ipynb)\n",
    "# Práctica 7: Residual neural networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zjhho6Jb6-j"
   },
   "source": [
    "### Pre-requisitos. Instalar paquetes\n",
    "\n",
    "Para la primera parte de este Laboratorio 7 necesitaremos TensorFlow y TensorFlow-Datasets. Además, como habitualmente, fijaremos la semilla aleatoria para asegurar la reproducibilidad de los experimentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjX2mh1-GSga"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "#Fijamos la semilla para poder reproducir los resultados\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "seed=1234\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyTV4jfNeiRk"
   },
   "source": [
    "Además, cargamos también APIs que vamos a emplear para que el código quede más legible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o3-nLMx-enlq"
   },
   "outputs": [],
   "source": [
    "#API de Keras, modelo Sequential y la capa Dense \n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense \n",
    "#Para mostrar gráficas\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xd9GpQD5WVY4"
   },
   "source": [
    "### Carga del conjunto de datos\n",
    "\n",
    "De nuevo, seguimos empleando el conjunto *german_credit_numeric* ya empleado en los laboratorios anteriores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpsZlN_mEj7D"
   },
   "outputs": [],
   "source": [
    "# TODO: Carga el conjunto german_credit como ds_train\n",
    "# Indica además un tamaño de batch de 128 y que se repita indefinidamente\n",
    "ds_train = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización del desvanecimiento del gradiente\n",
    "Las Residual neural networks se ocupan de que los gradientes no se desvanezcan cuando la red es muy profunda. Vamos a visualizar este problema creando una red profunda y mostrando las dimensiones de los gradientes que llegan a cada capa.\n",
    "\n",
    "Para ello debemos registrar las dimensiones de los gradientes a lo largo de entrenamiento. Crearemos un nuevo tipo de modelo, que va registrando los gradientes a medida que es entrenado. Nuestra nueva clase heredará de `tf.keras.models.Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oXz3n9daGXjG"
   },
   "outputs": [],
   "source": [
    "class GradientLoggingSequentialModel(tf.keras.models.Sequential):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # En la inicialización instanciamos una nueva variable en la que\n",
    "        # registraremos el historial de tamaños de gradientes de cada capa\n",
    "        self.gradient_history = {}\n",
    "    \n",
    "    def compile(self, **kwargs):\n",
    "        result = super().compile(**kwargs)\n",
    "        # Una vez sabemos la arquitectura, podemos inicializar la historia\n",
    "        # de gradientes de cada capa a una lista vacía.\n",
    "        for l in self.layers:\n",
    "            self.gradient_history[l.name] = []\n",
    "        return result\n",
    "        \n",
    "    def _save_gradients(self, gradients):\n",
    "        # A cada paso de entrenamiento llamaremos a esta función para que\n",
    "        # registre los gradientes.\n",
    "        # En la lista gradients se encuentran los gradientes de las distintas\n",
    "        # capas por orden. Cada capa l tendrá un número de gradientes que\n",
    "        # concidirá con l.trainable_variables.\n",
    "        # Teniendo esto en cuenta, recorremos los gradientes, calculamos su\n",
    "        # tamaño y guardamos la media de tamaños de cada capa en el histórico\n",
    "        i = 0\n",
    "        for layer in self.layers:\n",
    "            gradient_sizes = []\n",
    "            for lw in layer.trainable_variables:\n",
    "                g_size = np.linalg.norm(gradients[i].numpy())\n",
    "                gradient_sizes.append(g_size)\n",
    "                i += 1\n",
    "            mean_gradient_size = np.mean(gradient_sizes)\n",
    "            self.gradient_history[layer.name].append(mean_gradient_size)\n",
    "        \n",
    "    def train_step(self, data):\n",
    "        # Haremos un paso de entrenamiento personalizado basado en \n",
    "        # https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit#a_first_simple_example\n",
    "        # Dejaremos el ejemplo tal cual, añadiendo tan solo la llamada a\n",
    "        # _save_gradients una vez que disponemos de los gradientes\n",
    "        \n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        x, y = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            # Compute the loss value\n",
    "            # (the loss function is configured in `compile()`)\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        \n",
    "        # Llamada añadida para grabar los gradientes.\n",
    "        self._save_gradients(gradients)\n",
    "        \n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENqWLaw7fuxl"
   },
   "source": [
    "### Creamos un modelo *GradientLoggingSequentialModel*\n",
    "Creamos un modelo *Sequential* para ajustar a los datos de entrada siguiendo las especificaciones dadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KrIOT-4DmRuu",
    "outputId": "79dba569-e5e5-426f-9a87-57757db7d363"
   },
   "outputs": [],
   "source": [
    "# TODO - Define en model una red GradientLoggingSequentialModel con 20 capas ocultas, con activación sigmoide, con 10 unidades por capa.\n",
    "\n",
    "model = ...\n",
    "\n",
    "#Construimos el modelo y mostramos \n",
    "model.build()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNSojR94dP3y"
   },
   "source": [
    "### Entrenamiento del modelo\n",
    "Vamos a establecer la función de pérdida (entropía cruzada binaria), el optimizador (SGD con LR $10^{-3}$) y la métrica que nos servirá para evaluar el rendimiento del modelo entrenado (área bajo la curva)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - Compila el modelo. Utiliza la opción run_eagerly=True para que se puedan registrar los gradientes a cada paso\n",
    "model.compile(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rx2nvo6G0Qvn"
   },
   "source": [
    "Entrenamos el modelo usando model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0EzQDyKfGfuu"
   },
   "outputs": [],
   "source": [
    "#TODO - entrenar el modelo utilizando 8 steps por epoch. Con 10 epochs nos valdrá para comprobar el desvanecimiento de gradientes.\n",
    "model.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pcjua_KlWymG"
   },
   "source": [
    "Ahora que hemos hecho algunos pasos de entrenamiento, representamos el tamaño medio de los pesos de cada capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora accedemos al historial de gradientes y representamos el tamaño medio de los gradientes de cada capa.\n",
    "pyplot.figure(figsize=(14, 6), dpi=80)\n",
    "pyplot.boxplot(model.gradient_history.values())\n",
    "pyplot.yscale('log')\n",
    "pyplot.xticks(ticks=range(1,len(model.gradient_history)+1), labels=model.gradient_history.keys())\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparativa\n",
    " - ¿Qué observas en los pesos?\n",
    " - ¿Ocurre lo mismo utilizando ReLU como función de activación de las capas ocultas?\n",
    " - Repite la prueba con las distintas [funciones de activación](https://keras.io/api/layers/activations/) que tengan sentido.\n",
    " - (OPCIONAL) Alarga el entrenamiento y prueba distintos optimizadores para intentar que el modelo entrene correctamente."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab4_parte1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
