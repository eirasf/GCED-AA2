{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57knM8jrYZ2t"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eirasf/GCED-AA2/blob/main/lab2/lab2-parte2.ipynb)\n",
    "\n",
    "# Práctica 1: Redes neuronales desde cero con TensorFlow - Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta segunda parte de la práctica vamos a utilizar TensorFlow para implementar y entrenar la misma red neuronal que desarrollamos con Numpy en la parte 1.\n",
    "\n",
    "Necesitaremos, por tanto, la librería TensorFlow además de las ya utilizadas Numpy y tensorflow_datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LkaimNJfYZ2w"
   },
   "outputs": [],
   "source": [
    "# COLAB - Para ejecutar desde Colab, descomentar la siguiente línea\n",
    "%tensorflow_version 2.x\n",
    "# LOCAL - Para ejecutar en Local, descomentar la siguiente línea\n",
    "#!pip3 install tensorflow numpy tensorflow-datasets\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Establecemos una semilla aleatoria para que los resultados sean reproducibles en distintas ejecuciones\n",
    "np.random.seed(1234567)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargaremos el conjunto de datos `german_credit_numeric` del que tomaremos el primer lote de 100 elementos, tal como hicimos en la parte 1 de la práctica. Obtendremos dos tensores (`vectores_x` y `etiquetas`) que serán los que utilizaremos posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Cargamos el conjunto de datos\n",
    "ds = tfds.load('german_credit_numeric', split='train')\n",
    "\n",
    "tamano_lote = 100\n",
    "\n",
    "elems = ds.batch(tamano_lote)\n",
    "lote_entrenamiento = None\n",
    "for elem in elems:\n",
    "    lote_entrenamiento = elem\n",
    "    break\n",
    "    \n",
    "vectores_x = tf.cast(lote_entrenamiento[\"features\"], dtype=tf.float64)\n",
    "etiquetas = tf.cast(lote_entrenamiento[\"label\"], dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaración del modelo\n",
    "\n",
    "En primer lugar, debemos crear en TensorFlow el grafo de operaciones que representa nuestro modelo. Para ello:\n",
    " 1. Creamos las variables que TF optimizará, es decir, los parámetros del modelo.\n",
    " 1. Creamos el grafo de operaciones que producen la predicción a partir de la entrada y las variables. En este caso utilizaremos funciones que relacionen variables de TF con tensores que contendrán datos utilizando operaciones de TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables auxiliares\n",
    "tamano_entrada = 24\n",
    "h0_size = 5\n",
    "h1_size = 3\n",
    "\n",
    "# CREACIÓN DE LAS VARIABLES\n",
    "# TODO - Completa las dimensiones de las matrices\n",
    "W0 = tf.Variable(np.random.randn(, ), dtype=tf.float64, name='W0')\n",
    "b0 = tf.Variable(np.random.randn(, ),dtype=tf.float64, name='b0')\n",
    "W1 = tf.Variable(np.random.randn(, ), dtype=tf.float64, name='W1')\n",
    "b1 = tf.Variable(np.random.randn(, ),dtype=tf.float64, name='b1')\n",
    "W2 = tf.Variable(np.random.randn(, ), dtype=tf.float64, name='W2')\n",
    "b2 = tf.Variable(np.random.randn(, ),dtype=tf.float64, name='b2')\n",
    "\n",
    "# Guardamos todas las variables en una lista para posteriormente acceder a ellas fácilmente\n",
    "VARIABLES = [W0, b0, W1, b1, W2, b2]\n",
    "\n",
    "\n",
    "# CREACIÓN DEL GRAFO DE OPERACIONES\n",
    "@tf.function\n",
    "def capa_sigmoide(x, W, b):\n",
    "    # TODO - Completa con funciones de tensorflow el cálculo de la salida de una capa en la siguiente línea\n",
    "    return \n",
    "\n",
    "@tf.function\n",
    "def predice(x):\n",
    "    # TODO - Completa las siguientes líneas\n",
    "    h0 = \n",
    "    h1 = \n",
    "    y = \n",
    "    return y\n",
    "\n",
    "# Verificación\n",
    "x_test = np.random.randn(1,tamano_entrada)\n",
    "y_pred = predice(x_test) \n",
    "print(y_pred)\n",
    "np.testing.assert_almost_equal(0.48001507, y_pred.numpy(), err_msg='Revisa tu implementación')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo\n",
    "El modelo declarado ya se puede utilizar para hacer predicciones pasándole a la función `predice` un tensor con datos (tal como se ha hecho en el apartado de verificación de la celda anterior). Sin embargo, como vimos en la parte 1, este modelo no está ajustado a los datos de entrada, por lo que producirá malas predicciones.\n",
    "\n",
    "Debemos encontrar un conjunto de valores para los parámetros ($\\mathbf{W}_2$, $b_2$, $\\mathbf{W}_1$, $\\mathbf{b}_1$, $\\mathbf{W}_0$ y $\\mathbf{b}_0$) que minimicen la función de coste. TensorFlow nos ayuda a optimizar este proceso.\n",
    "\n",
    "TensorFlow permite configurar el proceso de optimización, por lo que deberemos indicarle:\n",
    " 1. Qué función de pérdida queremos. En nuestro caso habíamos elegido \n",
    " 1. Qué método de optimización utilizar. Como en la parte 1, utilizaremos descenso de gradiente.\n",
    " \n",
    "Por el momento crearemos sendas variables para almacenar ambas configuraciones. Al estar organizado de esta manera, utilizar una función de pérdida distinta o un algoritmo de optimización diferente será tan sencillo como cambiar estas variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_perdida = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "optimizador = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "#optimizador = tf.keras.optimizers.Adam(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El bucle de entrenamiento\n",
    "\n",
    "El bucle de entrenamiento será análogo al utilizado en la parte 1. Consistirá en ejecutar un número preestablecido (`NUM_EPOCHS`) de pasos de entrenamiento. En cada paso haremos lo siguiente:\n",
    " 1. Tomar los datos de entrada y calcular las predicciones que hace el modelo en su estado actual\n",
    " 1. Calcular el coste (la media de las pérdidas de cada predicción)\n",
    " 1. Utilizar el valor de coste para actualizar cada variable en dirección de su gradiente\n",
    "\n",
    "Crearemos una función `paso_entrenamiento` que realice este trabajo. TensorFlow se ocupará de calcular los gradientes y realizar las actualizaciones de las variables. Para calcular los gradientes, TensorFlow utiliza un `GradientTape`. Todas las operaciones con tensores que se realicen dentro del entorno en que está declarado este `GradientTape` quedarán registradas y eso nos permitirá obtener los gradientes directamente del `GradientTape` con una simple llamada. Puedes comprobar su funcionamiento en el ejemplo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def paso_entrenamiento(x, y):\n",
    "    # Declaración del GradientTape que registrará las operaciones\n",
    "    with tf.GradientTape() as tape:    \n",
    "        # TODO - Completa la siguiente línea para que calcule las predicciones\n",
    "        y_pred = \n",
    "        \n",
    "        # Cálculo de la pérdida utilizando la función que hemos escogido anteriormente\n",
    "        perdida = fn_perdida(y, y_pred)\n",
    "\n",
    "        # Consultar los gradientes es tan sencillo como indicarle dos cosas:\n",
    "        #    1. la función cuyo gradiente queremos obtener\n",
    "        #    2. la lista de variables respecto a las cuales queremos calcular el gradiente\n",
    "        # La función nos devolverá una lista con el gradiente correspondiente a cada variable de la lista\n",
    "        gradientes = tape.gradient(perdida, VARIABLES)\n",
    "        \n",
    "        # Realizar la actualización de las variables solo requiere esta llamada. Se le pasa una lista de tuplas (gradiente, variable)\n",
    "        optimizador.apply_gradients(zip(gradientes, VARIABLES))\n",
    "        \n",
    "        # Para poder mostrar la tasa de acierto, la calculamos a cada paso\n",
    "        fallos = tf.abs(tf.reshape(y,(tamano_lote, 1)) - y_pred)\n",
    "        tasa_acierto = tf.reduce_sum(1 - fallos)\n",
    "        \n",
    "        # Devolvemos estos dos valores para poder mostrarlos por pantalla cuando estimemos conveniente\n",
    "        return (perdida, tasa_acierto)\n",
    "\n",
    "# PROCESO DE ENTRENAMIENTO\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):    \n",
    "    perdida, tasa_error = paso_entrenamiento(vectores_x, etiquetas)\n",
    "    \n",
    "    if epoch % 100 == 99:\n",
    "        print(\"Epoch:\", epoch, 'Pérdida:', perdida.numpy(), 'Tasa de acierto:', tasa_error.numpy()/tamano_lote)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El uso de TensorFlow nos ha permitido abstraernos de los detalles de implementación y del cálculo de derivadas para centrarnos en la arquitectura de nuestro modelo."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "WBk0ZDWY-ff8"
   ],
   "name": "AA2 - Lab2 - Parte 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
